diff -Naur /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/main.F src/main.F
--- /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/main.F	2019-11-04 05:45:34.000000000 -0800
+++ src/main.F	2020-04-03 17:25:14.993489794 -0700
@@ -71,9 +71,11 @@
 ! (except for native 64-bit-REAL machines like CRAY style machines)
 !**********************************************************************
 
-      PROGRAM VAMP
-      USE prec
+!ZZ      PROGRAM VAMP
+      subroutine VAMP()
+      use mpi_wrapper
 
+      USE prec
       USE charge
       USE pseudo
       USE lattice
@@ -5061,6 +5063,7 @@
        WDES%NB_TOT,WDES%NB_TOT,NPL,NPRO,NRPLWV_RED,NPROD_RED,WDES%NB_TOT, &
        W%CPTWFP(:,:,NK,1),W%CPROJ(:,:,NK,1))
       CALL STOP_TIMING("G",IUT,'LINUP ')
-    END SUBROUTINE PERFORMANCE_TEST
+     END SUBROUTINE PERFORMANCE_TEST
 
-  END PROGRAM
+     !ZZ END PROGRAM
+     END subroutine 
diff -Naur /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/makefile src/makefile
--- /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/makefile	2019-11-04 05:45:34.000000000 -0800
+++ src/makefile	2020-04-10 13:00:33.154182000 -0700
@@ -134,15 +134,25 @@
 
 -include .depend
 
+### Begin MPI wrapper addition ###
+MPIW_PREFIX = m
+EXE_RENAMED = $(MPIW_PREFIX)$(EXE)
+FPP += -DINTEL 
+MPIW_OBJS = mpi_wrapper.o mwrapper.o
+
+$(MPIW_OBJS): %.o: %$(SUFFIX)
+	$(FC) $(FREE) $(FFLAGS) -c $*$(SUFFIX)
+### End MPI wrapper addition ###
+
 # export
 
 .PHONY: all cleanall clean sources dependencies depend libs $(LIB)
 
 all: libs sources
-	rm -f vasp ; $(MAKE) vasp ; cp vasp $(BINDIR)/$(EXE)
+	rm -f vasp ; $(MAKE) vasp ; cp vasp $(BINDIR)/$(EXE_RENAMED) ; mv vasp $(MPIW_PREFIX)vasp 
 	
-vasp: $(OBJS) $(FFT3D) $(INC) main.o
-	$(FCL) -o vasp $(OBJS) main.o $(FFT3D) $(LLIB) $(LINK)
+vasp: $(MPIW_OBJS) $(OBJS) $(FFT3D) $(INC) main.o
+	$(FCL) -o vasp $(OBJS) main.o $(MPIW_OBJS) $(FFT3D) $(LLIB) $(LINK)
 
 sources:
 	rsync -u $(SRCDIR)/*.F $(SRCDIR)/*.inc .
diff -Naur /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mpi.F src/mpi.F
--- /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mpi.F	2019-11-04 05:45:34.000000000 -0800
+++ src/mpi.F	2020-04-10 12:10:41.333350752 -0700
@@ -124,23 +124,27 @@
 !----------------------------------------------------------------------
 !
       SUBROUTINE M_init( COMM)
+!ZZ,1a
+      use mpi_wrapper
       IMPLICIT NONE
       INCLUDE "pm.inc"
 
      TYPE(communic) COMM
       INTEGER i, ierror
 
-      call MPI_init( ierror )
-      IF ( ierror /= MPI_success ) THEN
-         WRITE(*,*) 'Initpm: Error in MPI_init'
-        STOP
-      ENDIF
+!ZZ      call MPI_init( ierror )
+!ZZ      IF ( ierror /= MPI_success ) THEN
+!ZZ         WRITE(*,*) 'Initpm: Error in MPI_init'
+!ZZ        STOP
+!ZZ      ENDIF
+
 !
 ! initial communicator is world wide
 ! set only NCPU, NODE_ME and IONODE
 ! no internal setup done at this point
 !
-      COMM%MPI_COMM= MPI_comm_world
+!ZZ,1c      COMM%MPI_COMM= MPI_comm_world
+      COMM%MPI_COMM= aMPI_comm_world
 
       call MPI_comm_rank( COMM%MPI_COMM, COMM%NODE_ME, ierror )
       IF ( ierror /= MPI_success ) &
@@ -567,6 +571,8 @@
 !----------------------------------------------------------------------
 
       SUBROUTINE M_exit()
+!ZZ
+      use mpi_wrapper
       USE mpimy
       IMPLICIT NONE
       INCLUDE "pm.inc"
@@ -574,14 +580,16 @@
       TYPE(communic) COMM
       INTEGER i, ierror, id_in_group
 
-!      call MPI_barrier(MPI_comm_world, ierror )
-!      IF ( ierror /= MPI_success ) &
-!         CALL M_stop_ierr('Exitpm: Error in MPI_barrier',ierror)
+!     call MPI_barrier(MPI_comm_world, ierror )
+!     IF ( ierror /= MPI_success ) &
+!        CALL M_stop_ierr('Exitpm: Error in MPI_barrier',ierror)
 
-      call MPI_finalize( ierror )
-      IF ( ierror /= MPI_success ) &
-         CALL M_stop_ierr('Exitpm: Error in MPI_finalize',ierror)
-      STOP
+
+!ZZ      call MPI_finalize( ierror )
+!ZZ      IF ( ierror /= MPI_success ) &
+!ZZ        CALL M_stop_ierr('Exitpm: Error in MPI_finalize',ierror)
+!ZZ      STOP
+      ierror_app = 0
 
       RETURN
       END SUBROUTINE
@@ -595,6 +603,8 @@
 !----------------------------------------------------------------------
 
       SUBROUTINE M_stop(message)
+!ZZ,1a
+      use mpi_wrapper
       USE mpimy
       IMPLICIT NONE
       INCLUDE "pm.inc"
@@ -604,7 +614,9 @@
 
       WRITE (*,*) message
 
-      call MPI_abort(MPI_comm_world , 1, ierror )
+!ZZ      call MPI_abort(MPI_comm_world , 1, ierror )
+      ierror_app = 1 
+      call MPI_abort(aMPI_comm_world , 1, ierror )
       STOP
 
       RETURN
@@ -612,6 +624,8 @@
 
 
       SUBROUTINE M_stop_ierr(message, ierror)
+!ZZ              
+      use mpi_wrapper
       USE prec
       USE mpimy
       IMPLICIT NONE
@@ -622,7 +636,9 @@
 
       WRITE (*,*) message, ierror
 
-      call MPI_abort(MPI_comm_world , 1, ierror )
+!ZZ      call MPI_abort(MPI_comm_world , 1, ierror )
+      ierror_app = 1 
+      call MPI_abort(aMPI_comm_world , 1, ierror )
       STOP
 
       RETURN
diff -Naur /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mpi_wrapper.F src/mpi_wrapper.F
--- /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mpi_wrapper.F	1969-12-31 16:00:00.000000000 -0800
+++ src/mpi_wrapper.F	2020-04-09 16:05:13.182781000 -0700
@@ -0,0 +1,4 @@
+      module mpi_wrapper
+      integer :: aMPI_COMM_WORLD
+      integer :: ierror_app = 0
+      end Module mpi_wrapper
diff -Naur /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mwrapper.F src/mwrapper.F
--- /global/homes/z/zz217/vasp/vasp.5.4.4.pl2/src/mwrapper.F	1969-12-31 16:00:00.000000000 -0800
+++ src/mwrapper.F	2020-04-10 12:21:00.301740089 -0700
@@ -0,0 +1,270 @@
+!this program enables multiple instances of an application to run simultaneously.  
+program mwrapper 
+
+#ifdef INTEL
+  use ifport  
+#endif
+  use mpi_wrapper
+  implicit none
+  include 'mpif.h'
+  integer :: global_comm = MPI_COMM_WORLD, global_size, global_rank, njobs, color, key, size, rank, ierr
+  integer, parameter :: strlen = 200  
+  integer ::  ipartition = 0  
+  character(len=strlen) :: mydir, stdoutfile, stderrfile 
+#ifdef  SERIAL
+  character(len=strlen) :: appname 
+#endif
+  double precision ::  tstart, tend 
+
+! initial setup
+  call MPI_Init(ierr)
+  call MPI_Comm_rank(global_comm, global_rank, ierr)
+  call MPI_Comm_size(global_comm, global_size, ierr)
+
+! processing inputs
+#ifdef SERIAL 
+  call processing_input(global_comm, global_size, global_rank, njobs, appname, mydir, *500)
+#else
+  call processing_input(global_comm, global_size, global_rank, njobs, mydir, *500)
+#endif
+
+! splitting the processors into "njobs" number of groups, "color" is the gorup index
+  call groups(global_rank, global_size, njobs, color, key, ipartition)
+  call MPI_Comm_split(global_comm, color, key, aMPI_COMM_WORLD,ierr)
+  call MPI_Comm_rank(aMPI_COMM_WORLD, rank, ierr)
+  call MPI_Comm_size(aMPI_COMM_WORLD, size, ierr)
+
+  call print_msg(global_rank, 'njobs, global_size, ncores', 400)
+#ifdef SERIAL  
+  call print_msg(global_rank, 'application is ' // trim(appname), 499)
+#endif
+
+  call print_msg(rank, 'jobs starting', 401)
+
+! each job cd to its run directory 
+  ierr = chdir(mydir)
+
+! redirect the standard output (unit=6) to a file for each job 
+  write(stdoutfile, "('stdoutfile.', i3.3)") color
+  write(stderrfile, "('stderrfile.', i3.3)") color
+  if (rank == 0) open(unit = 6,file = stdoutfile, action = 'write')
+  if (rank == 0) open(unit = 0,file = stderrfile, action = 'write')
+  
+! run njobs jobs simultantously, one by each group
+  tstart = MPI_Wtime()
+#ifdef SERIAL 
+  call execute_command_line(appname, wait = .true., exitstat = ierror_app)
+#else
+!  call MPI_Comm_set_errhandler(aMPI_COMM_WORLD, MPI_ERRORS_RETURN, ierr) 
+  call vamp()  !can be any mpi application (need to convert to a subroutine first)  
+!  call jacobi_mpiomp()
+#endif
+  tend = MPI_Wtime()
+
+  if (rank == 0) close(unit = 6)
+  if (rank == 0) close(unit = 0)
+#ifdef GNU
+  if (rank == 0) open(unit = 6, file = "/dev/stdout",status='old')  !reconnect to stdout device
+#endif
+  call print_msg(rank, 'jobs ended', 402)
+
+!  call MPI_Barrier(global_comm,ierr)
+
+! Finish up
+500  call MPI_Finalize(ierr)
+
+  contains 
+
+! only rank 0 prints
+  subroutine print_msg(myrank, message, label)
+  integer, intent(in) :: myrank
+  character(*), intent(in), optional :: message
+  integer, intent(in) :: label
+  character(len=50) :: job_status
+
+400   format('running ', I4, ' jobs with ', I10, ' processors, ', I4, ' processors each.')
+401   format('the ', I4, '-th job with ', I4, ' processors running in directory, ', A, ' ...')  
+402   format('the ', I4, '-th job running in ', A,  ' is ', A, '. elapsed time (sec): ', F8.2)  
+
+  if (myrank == 0) then
+      select case (label)
+          case(400)
+              write(*, 400) njobs, global_size, global_size/njobs 
+          case(401)
+              write(*, 401) color, size, trim(mydir) 
+          case(402) 
+              job_status = 'completed'
+              if (ierror_app /= 0) job_status = 'failed'
+              write(*, 402) color, trim(mydir), trim(job_status), tend - tstart 
+          case default
+              write(*, *) message
+      end select
+  endif
+
+  return
+  end subroutine
+
+end program
+
+#ifdef SERIAL 
+  subroutine processing_input(global_comm, global_size, global_rank, njobs, appname, mydir, *)
+#else
+  subroutine processing_input(global_comm, global_size, global_rank, njobs, mydir, *)
+#endif
+  include 'mpif.h'
+  integer, intent(in) :: global_comm, global_size, global_rank
+  integer, intent(out) :: njobs
+  integer, parameter :: strlen = 200
+#ifdef SERIAL 
+  character(len = strlen), intent(out) :: appname 
+#endif
+  character(len = strlen), intent(out) :: mydir 
+  character(len = strlen), allocatable, dimension(:) :: rundirs
+  character(len = strlen) :: joblistfile
+  integer :: ierr, ierror, nlines, i
+  logical :: iexist
+
+! only the global rank 0 read in the joblist file
+  ierror = 0 
+  if (global_rank == 0) then
+
+!     defaults  
+      joblistfile = 'joblist.in'
+#ifdef SERIAL
+      appname = 'app.x'
+#endif
+!     get optional command line arguments
+      nargs = command_argument_count()
+      if (nargs >= 1)  call get_command_argument(1,joblistfile)
+#ifdef SERIAL
+      if (nargs >= 2)  call get_command_argument(2,appname)
+#endif
+
+      inquire(file = joblistfile, exist = iexist)
+      if (.not. iexist) then
+           print *, 'the joblistfile ', trim(joblistfile), ' does not exist, stop'
+           !call MPI_Abort(global_comm, 1, ierr) 
+           ierror = 1
+           goto 200
+      endif
+
+#ifdef SERIAL
+      call find_app(appname, ierr)
+      if (ierr /= 0) then
+           print *, 'the executable ', trim(appname), ' does not exist, stop'
+           !call MPI_Abort(global_comm, 1, ierr) 
+           ierror = 1
+           goto 200
+      endif
+#endif
+
+      open (10,file = trim(joblistfile), status = 'old', action = 'read',iostat = ierr)
+      rewind(10)
+      read(10,*) njobs
+      allocate(rundirs(0:njobs-1))
+      nlines = 0
+      do i = 0, njobs - 1
+          read(10, *, END = 100) rundirs(i)
+          nlines = nlines + 1
+      enddo
+
+100   continue
+      close(10)
+
+!     sanity checking:
+      if (nlines /= njobs) then
+           print *, 'the number of run diretories does not match the number of jobs provided &
+           &            on the first line of joblistfile, stop'
+           print *, 'nlines = ', nlines, ' njobs = ', njobs
+           !call MPI_Abort(global_comm, 1, ierr) 
+           ierror = 1
+           goto 200
+      endif
+
+      ncores = global_size / njobs
+      if ( ncores * njobs /= global_size ) then
+           print *, 'the total number of processors is not divisble by the number of jobs, stop ' 
+           print *, 'global_size, njobs = ', global_size, njobs  
+           !call MPI_Abort(global_comm, 1, ierr) 
+           ierror = 1
+           goto 200
+      endif
+
+      do i = 0, njobs - 1
+#ifdef INTEL
+          inquire(directory = rundirs(i), exist = iexist) 
+#else  
+          inquire(file = rundirs(i), exist = iexist)
+#endif
+          if (iexist .eqv. .false.) then
+               print *, 'the run directory ', trim(rundirs(i)), ' does not exit, stop'
+               !call MPI_Abort(global_comm, 1, ierr) 
+               ierror = 1
+               goto 200
+          endif
+      enddo
+  endif
+
+200 call MPI_Bcast(ierror, 1, MPI_integer, 0, global_comm, ierr)
+
+  if (ierror /= 0 ) return 1  !stop upon error 
+
+! broadcast jobinfo for the rest of the ranks 
+  call MPI_Bcast(njobs, 1, MPI_integer, 0, global_comm, ierr)
+#ifdef SERIAL  
+  call MPI_Bcast(appname, strlen, MPI_character, 0, global_comm, ierr)
+#endif
+  if (global_rank == 0) then
+      mydir = rundirs(0)
+      do i = 1, global_size - 1
+         ii = i / ncores  !equivalent to color 
+         call MPI_Send(rundirs(ii), strlen, MPI_Character, i, i, global_comm,ierr)
+      enddo
+  else
+      call MPI_Recv(mydir, strlen, MPI_Character, 0, global_rank, global_comm, status, ierr)
+  endif
+
+  if (global_rank == 0) then
+      deallocate(rundirs)
+  endif
+
+  return
+
+end subroutine
+
+#ifdef SERIAL
+! get the absolute path to the application
+  subroutine find_app(app, ierr)
+      character(*), intent(inout) :: app
+      integer, intent(out) :: ierr
+
+      call execute_command_line('which ' // app // '> /tmp/fullpath.file 2>/dev/null', wait = .true., exitstat = ierr)
+      if (ierr == 0) then
+          open(unit = 12, file = '/tmp/fullpath.file', status = 'old', action = 'read', iostat = ierr)
+          read(12, "(A)") app  ! "(A)" works but not "*"
+          close(12)
+          call execute_command_line('rm -fr /tmp/fullpath.file', wait=.false.)
+      endif
+
+      return
+  end subroutine
+#endif
+
+! different ways of splitting processors into sub groups can be provided here 
+subroutine groups(global_rank, global_size, njobs, color, key, ischeme)
+    integer, intent(in) :: global_rank, global_size, njobs, ischeme
+    integer, intent(out) :: color, key 
+    integer :: ncores
+
+    if (ischeme == 0) then
+        ncores = global_size / njobs
+        color = global_rank / ncores 
+        key = global_rank - color * ncores
+    else 
+        color = mod(global_rank, njobs)
+        key = int(global_rank / njobs)
+    endif
+
+    return
+end subroutine groups 
+
